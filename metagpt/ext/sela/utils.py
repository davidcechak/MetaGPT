# /tmp/MetaGPT_fork_sela/metagpt/ext/sela/utils.py
import os
import re
from datetime import datetime
from pathlib import Path
import traceback
import sys

import nbformat
import yaml
from loguru import logger as _logger
# Ensure Role can be imported or use a placeholder if it's complex to set up standalone
try:
    from metagpt.roles.role import Role
except ImportError:
    print("WARN: utils.py: Could not import 'Role' from 'metagpt.roles.role'. Using placeholder.")
    Role = object 

_UTILS_DIR = Path(__file__).parent

def load_data_config(file_name_or_path="data.yaml"): # Changed to file_name_or_path
    # Try path relative to utils.py first
    config_file_path = _UTILS_DIR / file_name_or_path
    if not config_file_path.exists():
        # Fallback to path relative to Current Working Directory
        print(f"WARN: utils.py: Config file '{config_file_path}' not found. Trying '{file_name_or_path}' from CWD.")
        config_file_path = Path(file_name_or_path)
        if not config_file_path.exists():
            print(f"ERROR: utils.py: Config file '{file_name_or_path}' not found relative to utils.py or CWD. Returning empty config.")
            return {}
    try:
        with open(config_file_path, "r") as stream:
            config = yaml.safe_load(stream)
        return config if config else {} # Handle empty YAML file
    except Exception as e:
        print(f"ERROR: utils.py: Failed to load/parse YAML from '{config_file_path}': {e}")
        traceback.print_exc()
        return {}

# Load base configurations
DATA_CONFIG = load_data_config("data.yaml")  # e.g., for role_dir, or other base settings
DATASET_METADATA_CONFIG = load_data_config("datasets.yaml") # For user_requirements, metrics per dataset

# --- SELA Specific Path Configuration & Overrides ---
# These paths are critical for SELA operations and will override any values
# loaded from data.yaml if they conflict, ensuring consistency.

# 1. Input datasets directory (where raw data like train.csv is located)
SELA_INPUT_DATASETS_DIR = "/repository/datasets/"
DATA_CONFIG["datasets_dir"] = SELA_INPUT_DATASETS_DIR
print(f"INFO: utils.py: SELA 'datasets_dir' (for raw input) is set to: {SELA_INPUT_DATASETS_DIR}")

# 2. Main SELA working directory (for temporary files, run outputs, logs etc.)
SELA_MAIN_WORK_DIR = "/tmp/sela/workspace" # Consistent with dataset.py's previous hardcoding
DATA_CONFIG["work_dir"] = SELA_MAIN_WORK_DIR
print(f"INFO: utils.py: SELA 'work_dir' is set to: {SELA_MAIN_WORK_DIR}")

# 3. Output directory for processed datasets (splits, info.json) generated by dataset.py
SELA_PROCESSED_OUTPUT_DIR = str(Path(SELA_MAIN_WORK_DIR) / "sela_datasets_output")
DATA_CONFIG["processed_datasets_output_dir"] = SELA_PROCESSED_OUTPUT_DIR
print(f"INFO: utils.py: SELA 'processed_datasets_output_dir' is set to: {SELA_PROCESSED_OUTPUT_DIR}")

# 4. Role directory (usually relative to work_dir for other MetaGPT components)
# Keep value from data.yaml if present, otherwise use SELA default.
DATA_CONFIG.setdefault("role_dir", "storage/SELA")
print(f"INFO: utils.py: SELA 'role_dir' is set to: {DATA_CONFIG['role_dir']}")

# 5. Merge dataset-specific metadata from datasets.yaml into DATA_CONFIG
DATA_CONFIG["datasets"] = DATASET_METADATA_CONFIG.get("datasets", {})
print(f"INFO: utils.py: Loaded {len(DATA_CONFIG.get('datasets', {}))} dataset metadata entries from datasets.yaml.")

# --- End of SELA Specific Path Configuration ---

# Standard SELA constants that might be needed by other modules (can be expanded)
# These were previously defined in dataset.py; utils.py is a more central place if they are shared.
DEFAULT_DATASETS_YAML_PATH = _UTILS_DIR / "datasets.yaml" # Path to the SELA datasets.yaml

# (Original functions from your provided utils.py, ensure they use the configured DATA_CONFIG)
def get_mcts_logger():
    logfile_level = DATA_CONFIG.get("logfile_level", "DEBUG") # Example of using DATA_CONFIG
    name: str = None
    current_date = datetime.now()
    formatted_date = current_date.strftime("%Y%m%d")
    log_name = f"{name}_{formatted_date}" if name else formatted_date

    _logger.remove() # Remove default handlers to avoid duplicate logs if re-called
    _logger.level("MCTS", color="<green>", no=25) # Define custom level if needed

    # Ensure work_dir and role_dir are correctly resolved
    log_dir_base = Path(DATA_CONFIG.get("work_dir", ".")) / DATA_CONFIG.get("role_dir", "logs")
    log_dir_base.mkdir(parents=True, exist_ok=True)
    log_file_path = log_dir_base / f"{log_name}.txt"

    _logger.add(sys.stderr, level="INFO") # Example: stderr logging at INFO
    _logger.add(log_file_path, level=logfile_level)
    print(f"INFO: utils.py: MCTS Logger configured. Log file: {log_file_path} (Level: {logfile_level})")
    _logger.propagate = False
    return _logger

mcts_logger = get_mcts_logger() # Initialize once

def get_exp_pool_path(task_name, data_config_param, pool_name="analysis_pool"):
    # This function refers to where an "analysis_pool.json" might be stored.
    # If this pool is related to the *raw input*, then using data_config_param["datasets_dir"] is correct.
    # If it's an *output* of some SELA process, it should use "processed_datasets_output_dir".
    # Assuming it's related to input for now.
    input_datasets_dir = data_config_param.get("datasets_dir") 
    if not input_datasets_dir:
        raise ValueError("'datasets_dir' (for input) not in data_config_param for get_exp_pool_path")

    # Assuming task_name is a direct subfolder under input_datasets_dir
    dataset_specific_input_path = Path(input_datasets_dir) / task_name
    exp_pool_file_path = dataset_specific_input_path / f"{pool_name}.json"
    
    print(f"INFO: utils.py: Looking for exp_pool '{pool_name}.json' at: {exp_pool_file_path}")
    if not exp_pool_file_path.exists():
        return None
    return str(exp_pool_file_path)

# (Keep other functions from your utils.py: change_plan, is_cell_to_delete, process_cells, save_notebook, load_execute_notebook, clean_json_from_rsp)
# Ensure they are compatible with any changes or rely on the now consistently configured DATA_CONFIG.

def change_plan(role, plan):
    print(f"Change next plan to: {plan}")
    tasks = role.planner.plan.tasks
    finished = True
    for i, task in enumerate(tasks):
        if not task.code:
            finished = False
            break
    if not finished and i < len(tasks): # ensure i is valid index
        tasks[i].plan = plan
    return finished

def is_cell_to_delete(cell: NotebookNode) -> bool:
    if "outputs" in cell:
        for output in cell["outputs"]:
            if output and "traceback" in output:
                return True
    return False

def process_cells(nb: NotebookNode) -> NotebookNode:
    new_cells = []
    i = 1
    for cell in nb["cells"]:
        if cell["cell_type"] == "code" and not is_cell_to_delete(cell):
            cell["execution_count"] = i
            new_cells.append(cell)
            i = i + 1
    nb["cells"] = new_cells
    return nb

def save_notebook(role: Role, save_dir: str = "", name: str = "", save_to_depth=False):
    save_path_base = Path(save_dir) if save_dir else Path(DATA_CONFIG.get("work_dir", ".")) / "notebook_outputs"
    save_path_base.mkdir(parents=True, exist_ok=True)
    
    tasks = role.planner.plan.tasks # Assuming role.planner.plan exists
    nb = process_cells(role.execute_code.nb) # Assuming role.execute_code.nb exists
    
    file_name_final = name if name else f"notebook_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    file_path_full = save_path_base / f"{file_name_final}.ipynb"
    nbformat.write(nb, file_path_full)
    print(f"INFO: utils.py: Notebook saved to {file_path_full}")

    if save_to_depth:
        clean_file_path_full = save_path_base / f"{file_name_final}_clean.ipynb"
        codes = [task.code for task in tasks if task.code]
        clean_nb = nbformat.v4.new_notebook()
        for code_block in codes: # Renamed to avoid conflict
            clean_nb.cells.append(nbformat.v4.new_code_cell(code_block))
        nbformat.write(clean_nb, clean_file_path_full)
        print(f"INFO: utils.py: Cleaned notebook saved to {clean_file_path_full}")


async def load_execute_notebook(role): # Assuming role is passed and has execute_code, planner
    tasks = role.planner.plan.tasks
    codes = [task.code for task in tasks if task.code]
    executor = role.execute_code
    executor.nb = nbformat.v4.new_notebook()
    # Assuming role.role_timeout is defined elsewhere or has a default.
    timeout_val = getattr(role, 'role_timeout', 600) # Default timeout 600s
    executor.nb_client = NotebookClient(executor.nb, timeout=timeout_val)
    
    print(f"INFO: utils.py: Preparing to execute loaded notebook with {len(codes)} code cells.")
    for idx, code_block in enumerate(codes):
        print(f"INFO: utils.py: Executing block {idx+1}/{len(codes)}")
        outputs, success = await executor.run(code_block)
        print(f"  Execution success: {success}")
        if not success:
            print(f"  Block {idx+1} FAILED. Output: {outputs}")
            break # Stop on first failure
    print("INFO: utils.py: Finished executing loaded notebook.")
    return executor

def clean_json_from_rsp(text):
    if not isinstance(text, str): return ""
    pattern = r"```json(.*?)```"
    matches = re.findall(pattern, text, re.DOTALL)
    if matches:
        json_str = "".join(matches).strip() # Changed from \n to "" for join
        # Basic check for valid JSON structure
        if (json_str.startswith('{') and json_str.endswith('}')) or \
           (json_str.startswith('[') and json_str.endswith(']')):
            print("INFO: utils.py: Extracted JSON from ```json ... ``` block.")
            return json_str
        else:
            print("WARN: utils.py: Found ```json ... ``` block but content doesn't look like valid JSON.")
            return "" # Or return the content as is, or text, depending on desired handling
    else: # If no ```json block, check if the whole text is a JSON
        text_stripped = text.strip()
        if (text_stripped.startswith('{') and text_stripped.endswith('}')) or \
           (text_stripped.startswith('[') and text_stripped.endswith(']')):
            # Try to parse to be more certain, but be careful with recursion or complex validation here
            try:
                json.loads(text_stripped) # Quick validation
                print("INFO: utils.py: Text itself appears to be a valid JSON string.")
                return text_stripped
            except json.JSONDecodeError:
                print("WARN: utils.py: Text seemed like JSON but failed to parse.")
                return "" # Or return text_stripped if partial JSON is okay
        return "" # Default to empty if no clear JSON found

print("INFO: utils.py: SELA utils module loaded and DATA_CONFIG configured with SELA specific paths.")